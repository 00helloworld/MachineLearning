{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a optimization algorithm to minimize the cost function.  \n",
    "We need to find out the parameter $w$ to make the cost function minimal.  \n",
    "\n",
    "For example, assume:\n",
    "$$error(x) = w^2x$$  \n",
    "We need to find a $w$ to minimize $error(x) = w^2x$  \n",
    "So we can calculate the gradient of $w$ which is $\\nabla w$\n",
    "Then we update $w$ with:\n",
    "$$w = w - \\alpha \\nabla w$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use gradient descent to find minima of $f(x)=x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=5, gradient=10, new_x=3.0, iter=0\n",
      "x=3.0, gradient=6.0, new_x=1.7999999999999998, iter=1\n",
      "x=1.7999999999999998, gradient=3.5999999999999996, new_x=1.0799999999999998, iter=2\n",
      "x=1.0799999999999998, gradient=2.1599999999999997, new_x=0.6479999999999999, iter=3\n",
      "x=0.6479999999999999, gradient=1.2959999999999998, new_x=0.3887999999999999, iter=4\n",
      "x=0.3887999999999999, gradient=0.7775999999999998, new_x=0.23327999999999993, iter=5\n",
      "x=0.23327999999999993, gradient=0.46655999999999986, new_x=0.13996799999999995, iter=6\n",
      "x=0.13996799999999995, gradient=0.2799359999999999, new_x=0.08398079999999997, iter=7\n",
      "x=0.08398079999999997, gradient=0.16796159999999993, new_x=0.05038847999999998, iter=8\n",
      "x=0.05038847999999998, gradient=0.10077695999999996, new_x=0.030233087999999984, iter=9\n",
      "x=0.030233087999999984, gradient=0.06046617599999997, new_x=0.018139852799999988, iter=10\n",
      "x=0.018139852799999988, gradient=0.036279705599999976, new_x=0.010883911679999993, iter=11\n",
      "x=0.010883911679999993, gradient=0.021767823359999987, new_x=0.006530347007999996, iter=12\n",
      "x=0.006530347007999996, gradient=0.013060694015999992, new_x=0.0039182082047999976, iter=13\n",
      "x=0.0039182082047999976, gradient=0.007836416409599995, new_x=0.0023509249228799984, iter=14\n",
      "x=0.0023509249228799984, gradient=0.004701849845759997, new_x=0.001410554953727999, iter=15\n",
      "x=0.001410554953727999, gradient=0.002821109907455998, new_x=0.0008463329722367994, iter=16\n",
      "x=0.0008463329722367994, gradient=0.0016926659444735988, new_x=0.0005077997833420796, iter=17\n",
      "x=0.0005077997833420796, gradient=0.0010155995666841593, new_x=0.0003046798700052478, iter=18\n",
      "x=0.0003046798700052478, gradient=0.0006093597400104956, new_x=0.00018280792200314866, iter=19\n",
      "x=0.00018280792200314866, gradient=0.0003656158440062973, new_x=0.0001096847532018892, iter=20\n",
      "break\n",
      "y=3.341873634710928e-08\n"
     ]
    }
   ],
   "source": [
    "def gd(learning_rate=0.2, max_iter=100, initial_x=5, tolerance=0.0001):\n",
    "    x = initial_x\n",
    "    for i in range(max_iter):\n",
    "        gradient = 2 * x\n",
    "        new_x = x - learning_rate * gradient\n",
    "        print(f'x={x}, gradient={gradient}, new_x={new_x}, iter={i}')\n",
    "        if abs(new_x - x) < tolerance:\n",
    "            print('break')\n",
    "            break\n",
    "        x = new_x\n",
    "    \n",
    "    print(f'y={pow(x, 2)}')\n",
    "\n",
    "gd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
